from os.path import exists
from typing import List, Dict

import cv2
import face_recognition
import numpy as np
import hdbscan
from fastapi import UploadFile
from collections import deque
from scipy.spatial.distance import cosine
from sklearn.metrics.pairwise import pairwise_distances

from src.utils.file_io import load_json, save_json
from src.services.photo.storage import save_image_to_album
from src.constants import (
    METADATA_PATH,
    REPRESENTATIVES_PATH,
    TEMP_CLUSTER_PATH,
    TEMP_ENCODING_PATH,
    RECENT_VECTOR_COUNT,
)


# ì¶”ê°€ ì‚¬ì§„ í´ëŸ¬ìŠ¤í„°ë§ - KNN ë°©ì‹
async def add_incremental_faces(files: List[UploadFile]) -> Dict:
    face_image_map = load_json(TEMP_ENCODING_PATH, [])
    clustered_result = load_json(TEMP_CLUSTER_PATH, {})
    results = []
    saved_files = {}

    for file in files:
        image_bytes = await file.read()
        image_np = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(image_np, cv2.IMREAD_COLOR)

        face_locations = face_recognition.face_locations(image)
        encodings = face_recognition.face_encodings(image, face_locations)
        
        saved_filename = save_image_to_album(file, image)

        for encoding, loc in zip(encodings, face_locations):
            if is_duplicate_face(encoding, loc):
                print(f"ì¤‘ë³µ ì–¼êµ´ ê±´ë„ˆëœ€: {file.filename}, ìœ„ì¹˜ {loc}")
                continue

            # ìµœê·¼ì ‘ ëŒ€í‘œ ë²¡í„° ê¸°ë°˜ ë¶„ë¥˜
            person_id = find_nearest_person(
                encoding, file.filename, loc, save_to_storage=False  # TEMPì—ë§Œ ì €ì¥
            )

            # ì–¼êµ´ ID ìƒì„±
            face_id = get_next_temp_face_id(face_image_map)

            # TEMP ì €ì¥ìš© ë ˆì½”ë“œ
            face_record = {
                "file_name": saved_filename,
                "location": loc,
                "face_id": face_id,
                "predicted_person": person_id,
                "encoding": encoding.tolist(),
            }

            # TEMP_ENCODING_PATHì— ì €ì¥
            face_image_map.append(face_record)

            # TEMP_CLUSTER_PATHì— ì €ì¥
            clustered_result.setdefault(person_id, []).append(
                {
                    "file_name": saved_filename,
                    "location": loc,
                    "face_id": face_id,
                }
            )
            results.append(
                {
                    "predicted_person": person_id,
                    "location": loc,
                    "file_name": saved_filename,
                }
            )

    # ì €ì¥
    save_json(TEMP_ENCODING_PATH, face_image_map)
    save_json(TEMP_CLUSTER_PATH, clustered_result)

    return {"num_faces": len(results), "results": results}


# í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë§Œ ë°˜í™˜ (ì €ì¥ì€ ì•ˆí•¨) - ë¹„ì§€ë„ í•™ìŠµ ê¸°ë°˜, HDBSCAN
async def run_album_clustering(files: List[UploadFile]) -> Dict:
    all_face_encodings = []  # ì „ì²´ ì–¼êµ´ ë²¡í„°
    face_image_map = []  # ì–¼êµ´ ë²¡í„°ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ ì •ë³´ (íŒŒì¼ëª…, ì–¼êµ´ ì¢Œí‘œ)
    saved_files = {}
    raw_images = {}

    for file in files:
        image_bytes = await file.read()
        image_np = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(image_np, cv2.IMREAD_COLOR)
        raw_images[file.filename] = (file, image)

        face_locations = face_recognition.face_locations(image)
        encodings = face_recognition.face_encodings(image, face_locations)

        for loc, encoding in zip(face_locations, encodings):
            if is_duplicate_face(encoding, loc):
                print(f"ì¤‘ë³µ ì–¼êµ´ ê±´ë„ˆëœ€: {file.filename}, ìœ„ì¹˜ {loc}")
                continue

            all_face_encodings.append(encoding)
            face_image_map.append(
                {
                    "file_name": file.filename,
                    "location": loc,  # (top, right, bottom, left)
                    "encoding": encoding.tolist(),  # ë‹¤ìŒ ë‹¨ê³„ì— ì „ë‹¬
                }
            )

    if not all_face_encodings:
        return {"message": "ë“±ë¡ëœ ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤."}

    # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    distance_matrix = pairwise_distances(all_face_encodings, metric="cosine")
    clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric="precomputed")
    labels = clusterer.fit_predict(distance_matrix)

    clustered_result = {}  # ì‚¬ì§„ ìœ„ì¹˜ ì •ë³´ ì €ì¥
    used_ids = []
    if exists(TEMP_CLUSTER_PATH):
        try:
            prev_data = load_json(TEMP_CLUSTER_PATH)
            for person_faces in prev_data.values():
                for face in person_faces:
                    fid = face.get("face_id")
                    if fid and fid.startswith("face_"):
                        used_ids.append(int(fid.replace("face_", "")))
        except:
            pass

    current_id = max(used_ids + [-1]) + 1
    cluster_vectors = {}  # ì‹¤ì œ ì–¼êµ´ ë²¡í„° ë°ì´í„° ì €ì¥ (ê³„ì‚°ìš© ë°ì´í„°)

    for idx, label in enumerate(labels):
        info = face_image_map[idx]
        cluster_key = "noise" if label == -1 else f"person_{label}"

        face_id = f"face_{current_id:04}"
        current_id += 1

        info["face_id"] = face_id
        info["predicted_person"] = cluster_key

        # íŒŒì¼ ì €ì¥ (person_id ê¸°ì¤€ ì´ë¦„ìœ¼ë¡œ)
        original_filename = info["file_name"]

        file, image = raw_images[original_filename]
        saved_filename = save_image_to_album(file, image)
        saved_files[original_filename] = saved_filename

        info["file_name"] = saved_filename

        # ì €ì¥ ëŒ€ìƒ í•„ë“œë§Œ ë°˜í™˜ (encoding ì œì™¸)
        clustered_result.setdefault(cluster_key, []).append(
            {
                "file_name": info["file_name"],
                "location": info["location"],
                "face_id": face_id,
            }
        )

        # ë²¡í„° ì €ì¥ (noiseëŠ” ì œì™¸)
        if cluster_key != "noise":
            cluster_vectors.setdefault(cluster_key, []).append(all_face_encodings[idx])

    # ì´ì „ override ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜´
    try:
        previous_data = load_json(TEMP_CLUSTER_PATH)
    except FileNotFoundError:
        previous_data = {}

    # override ì •ë³´ë¥¼ clustered_resultì— ë³‘í•©
    for person_key, faces in clustered_result.items():
        for face in faces:
            target_file = face.get("file_name")
            target_loc = face.get("location")
            # ê¸°ì¡´ temp ë°ì´í„°ì— overrideê°€ ì¡´ì¬í•  ê²½ìš° ë®ì–´ì“°ê¸°
            for prev_faces in previous_data.values():
                for prev_face in prev_faces:
                    if (
                        prev_face.get("file_name") == target_file
                        and prev_face.get("location") == target_loc
                        and prev_face.get("override")
                    ):
                        face["override"] = prev_face["override"]

    # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë²¡í„°(í‰ê· ê°’) ì €ì¥
    representatives = {}
    for person_id, vectors in cluster_vectors.items():
        mean_vector = np.mean(vectors, axis=0)
        representatives[person_id] = mean_vector.tolist()

    save_json(REPRESENTATIVES_PATH, representatives)

    # ì„ì‹œ ì €ì¥
    save_json(TEMP_CLUSTER_PATH, clustered_result)
    save_json(TEMP_ENCODING_PATH, face_image_map)

    return {
        "num_faces": len(all_face_encodings),
        "num_clusters": len(set(labels)) - (1 if -1 in labels else 0),
        "num_noise": list(labels).count(-1),
        "clusters": clustered_result,
        "representatives_saved": True,
    }


# í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ì‹¤ì œë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_clustered_faces(
    cluster_data: Dict[str, List[Dict]], full_encodings: List[Dict]
) -> Dict:
    face_data = load_json(METADATA_PATH)

    new_faces = {}
    for person_id, faces in cluster_data.items():
        if person_id == "noise":
            continue  # ë…¸ì´ì¦ˆëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ

        for face in faces:
            file_name = face["file_name"]
            location = face["location"]

            # encoding ì°¾ê¸°
            encoding = next(
                (
                    e["encoding"]
                    for e in full_encodings
                    if e["file_name"] == file_name and e["location"] == location
                ),
                None,
            )
            if encoding is None:
                continue  # ëª» ì°¾ìœ¼ë©´ skip

            face_id = get_next_face_id(face_data)
            face_data[face_id] = {
                "file_name": file_name,
                "location": location,
                "person_id": person_id,
                "encoding": (
                    encoding if isinstance(encoding, list) else encoding.tolist()
                ),
            }
            new_faces[face_id] = face_data[face_id]

    save_json(METADATA_PATH, face_data)
    return {"saved_faces": new_faces}


# ì¦ë¶„ ì¸ë¬¼ ë¶„ë¥˜ (KNN ë°©ì‹)
def find_nearest_person(
    new_encoding: np.ndarray,
    file_name: str,
    location,
    threshold: float = 0.45,
    save_to_storage: bool = True,
) -> str:
    # ì¤‘ë³µ ì–¼êµ´ ì²´í¬
    if is_duplicate_face(new_encoding, location):
        print(f"ì¤‘ë³µ ì–¼êµ´ ê°ì§€: {file_name}ì˜ ì–¼êµ´ì€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        return "duplicate_person"

    reps = load_json(REPRESENTATIVES_PATH)

    closest_person = None
    closest_dist = float("inf")

    # ì¶”ê°€ëœ ì‚¬ì§„ì˜ ë²¡í„°ì™€ ê¸°ì¡´ í‰ê·  ë²¡í„°ì™€ ê±°ë¦¬ ë¹„êµ
    for person_id, vector in reps.items():
        distance = cosine(new_encoding, vector)
        if distance < closest_dist:
            closest_person = person_id
            closest_dist = distance

    if closest_dist < threshold:
        person_id = closest_person
    else:
        person_id = get_new_person_id()

    # ì¡°ê±´ë¶€ ì €ì¥
    if save_to_storage:
        update_representative(person_id, new_encoding)
        add_face_record(new_encoding, file_name, location, person_id)

    return person_id


# ìƒˆë¡œìš´ ì–¼êµ´ ì¶”ê°€ í•¨ìˆ˜
def add_face_record(encoding: np.ndarray, file_name: str, location, person_id: str):
    # ì¤‘ë³µ ì–¼êµ´ì¸ì§€ í™•ì¸
    if is_duplicate_face(encoding, location):
        return  # ì¤‘ë³µì´ë©´ ì €ì¥í•˜ì§€ ì•Šê³  ì¢…ë£Œ

    face_data = load_json(METADATA_PATH)

    new_id = get_next_face_id(face_data)
    face_data[new_id] = {
        "file_name": file_name,
        "location": location,
        "person_id": person_id,
        "encoding": encoding.tolist(),
    }

    save_json(METADATA_PATH, face_data)


# ëŒ€í‘œ ë²¡í„° ê°±ì‹  í•¨ìˆ˜ (ìµœê·¼ Nê°œì˜ ë²¡í„°ë¥¼ í‰ê· )
def update_representative(person_id: str, new_encoding: np.ndarray):
    # ë²¡í„° íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
    if REPRESENTATIVES_PATH.exists():
        data = load_json(REPRESENTATIVES_PATH)
    else:
        data = {}

    # íˆìŠ¤í† ë¦¬ í‚¤ ì„¤ì •
    history_key = f"{person_id}_history"
    history_list = data.get(history_key, [])

    # dequeë¡œ ë³€í™˜í•´ì„œ ìµœëŒ€ ê¸¸ì´ ì œí•œ
    vector_history = deque(history_list, maxlen=RECENT_VECTOR_COUNT)
    vector_history.append(new_encoding.tolist())

    # ëŒ€í‘œ ë²¡í„°ëŠ” ìµœê·¼ Nê°œ í‰ê· 
    new_mean = np.mean(np.array(vector_history), axis=0)

    # ì €ì¥
    data[person_id] = new_mean.tolist()
    data[history_key] = list(vector_history)

    save_json(REPRESENTATIVES_PATH, data)


# ìƒˆë¡œìš´ ì‚¬ëŒ ID ìƒì„±
def get_new_person_id() -> str:
    reps = load_json(REPRESENTATIVES_PATH)
    existing = [
        int(k.replace("person_", "")) for k in reps.keys() if k.startswith("person_")
    ]
    next_id = max(existing + [-1]) + 1
    return f"person_{next_id}"


# ì–¼êµ´ ë‹¨ìœ„ ID ìƒì„±
def get_next_face_id(face_data: dict) -> str:
    existing_ids = [int(k.replace("face_", "")) for k in face_data.keys()]
    next_id = max(existing_ids, default=-1) + 1
    return f"face_{next_id:04}"


# ì–¼êµ´ ë‹¨ìœ„ ì„ì‹œ ID ìƒì„±
def get_next_temp_face_id(temp_face_data: list) -> str:
    existing_ids = [
        int(face["face_id"].replace("face_", ""))
        for face in temp_face_data
        if "face_id" in face
    ]
    next_id = max(existing_ids, default=-1) + 1
    return f"face_{next_id:04}"


# ì¤‘ë³µ ì–¼êµ´ ì²´í¬ (ìœ ì‚¬ë„ê°€ 0.95 ì´ìƒì¼ ë•Œë§Œ ì¤‘ë³µ ì²˜ë¦¬)
def is_duplicate_face(
    new_encoding: np.ndarray, location, threshold: float = 0.95
) -> bool:

    for source_path in [METADATA_PATH, TEMP_ENCODING_PATH]:
        face_data = load_json(source_path)

        if not face_data:
            return False

        for face_info in (
            face_data.values() if isinstance(face_data, dict) else face_data
        ):
            saved_encoding = np.array(face_info["encoding"])
            similarity = 1 - cosine(new_encoding, saved_encoding)

            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€ ğŸ‘‡
            print("â†’ ë¹„êµ ëŒ€ìƒ:", face_info["location"])
            print("â†’ í˜„ì¬ ì—…ë¡œë“œ:", location)
            print("â†’ similarity:", similarity)

            # ìœ„ì¹˜ê°€ ë™ì¼í•˜ê³ , ìœ ì‚¬ë„ ê¸°ì¤€ ì´ìƒì´ë©´ ì¤‘ë³µ ì²˜ë¦¬
            if face_info.get("location") == location and similarity > threshold:
                return True

            # ìœ ì‚¬ë„ë§Œìœ¼ë¡œ ì¤‘ë³µ íŒë‹¨
            if similarity > threshold:
                return True

    return False  # ì¤‘ë³µ ì•„ë‹˜


# ì‚¬ìš©ì ìˆ˜ì •ì‚¬í•­ ë°˜ì˜(override í•„ë“œ) ì¶”ê°€
def override_person(face_id: str, new_person_id: str) -> bool:
    # 1. ë¨¼ì € ì •ì‹ ì €ì¥ ë°ì´í„°ì—ì„œ ì°¾ê¸°
    face_data = load_json(METADATA_PATH)
    if face_id in face_data:
        face_data[face_id]["override"] = new_person_id
        save_json(METADATA_PATH, face_data)
        return True

    # 2. ì„ì‹œ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì—ì„œ ì°¾ê¸°
    if exists(TEMP_CLUSTER_PATH):
        temp_data = load_json(TEMP_CLUSTER_PATH)
        updated = False

        for person_key, faces in temp_data.items():
            for face in faces:
                if face.get("face_id") == face_id:
                    face["override"] = new_person_id
                    updated = True

        if updated:
            save_json(TEMP_CLUSTER_PATH, temp_data)
            return True

    return False  # ì–´ë””ì—ë„ ì—†ìŒ
